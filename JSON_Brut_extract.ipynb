{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulienDbrt/BGS-Invoice/blob/main/JSON_Brut_extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTjeM6Q9jCWD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BJ5bfGNcQTI"
      },
      "outputs": [],
      "source": [
        "!pip install openai mistralai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bq8kYB_D97w"
      },
      "source": [
        "# Skipped :\n",
        "64060\n",
        "63257\n",
        "63155\n",
        "63820\n",
        "63155\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "YRF7IwCv9VDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction de \"Pages\", nettoyage de la données et envoie dans OpenAI API (le modèle varie selon le nombre de tokens estimé)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import glob\n",
        "import json\n",
        "import openai\n",
        "import chardet\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/EXPORT_3EME_WEB 4'\n",
        "output_file_path = '/content/json_final_output_openai.csv'\n",
        "\n",
        "class Result(BaseModel):\n",
        "    vendorEmail: str\n",
        "    uo_2: str\n",
        "    invoiceNumber: str\n",
        "    vendorCode: str\n",
        "    commitmentCode: str\n",
        "    title: str\n",
        "    dueDate: str\n",
        "    documentType: str\n",
        "    vendorSiret: str\n",
        "    projectCode: str\n",
        "    ttc: str\n",
        "    tiers: str\n",
        "    ht: str\n",
        "    dateDocument: str\n",
        "    subChapter: str\n",
        "    commitment: str\n",
        "    project: str\n",
        "    invoiceType: str\n",
        "\n",
        "def estimate_token_count(text):\n",
        "    \"\"\"Estimate the number of tokens based on the text length.\n",
        "    Rough estimation assuming an average token length, including spaces.\"\"\"\n",
        "    return len(text) / 5\n",
        "\n",
        "def extract_data_json(json_file_path):\n",
        "    \"\"\"Extract and process data from a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(json_file_path, 'rb') as file:\n",
        "            raw_data = file.read()\n",
        "            if not raw_data:\n",
        "                print(f\"Warning: The file {json_file_path} is empty.\")\n",
        "                return None\n",
        "            encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'\n",
        "            json_data = json.loads(raw_data.decode(encoding))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from the file {json_file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    pages_data = json_data.get('pages', [])\n",
        "    sentences = []\n",
        "\n",
        "    for page in pages_data:\n",
        "        if page.get('$type') == 'PageContent':\n",
        "            for text_zone in page.get('Items', []):\n",
        "                if text_zone.get('$type') == 'TextZone':\n",
        "                    for line in text_zone.get('Ln', []):\n",
        "                        for item in line.get('Items', []):\n",
        "                            if item.get('$type') == 'Word':\n",
        "                                sentences.append(item.get('Value', ''))\n",
        "\n",
        "    sentence = ' '.join(sentences)\n",
        "    file_name = os.path.basename(json_file_path)\n",
        "    return {'sentence': sentence, 'file_name': file_name}\n",
        "\n",
        "def process_json_files(directory_path, output_file_path):\n",
        "    json_files = glob.glob(f\"{directory_path}/*.json\")\n",
        "    results = []\n",
        "    print(f\"Processing {len(json_files)} files...\")\n",
        "\n",
        "    for file_path in tqdm(json_files, desc=\"Progress\"):\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        invoice_data = extract_data_json(file_path)\n",
        "        if invoice_data is None:\n",
        "            continue\n",
        "\n",
        "        sentence = invoice_data.get('sentence', '')\n",
        "        token_count = estimate_token_count(sentence)\n",
        "        if token_count > 32000:\n",
        "            max_length = 32000 * 5\n",
        "            invoice_data['sentence'] = sentence[:max_length]\n",
        "\n",
        "        document_details = invoice_data\n",
        "\n",
        "        token_count = estimate_token_count(document_details)\n",
        "        print(f\"Estimated token count for {file_path}: {token_count}\")\n",
        "\n",
        "        model = 'gpt-4-0125-preview' if token_count > 15500 else 'gpt-3.5-turbo-0125'\n",
        "\n",
        "        if token_count > 31000:\n",
        "            print(f\"{file_path} skipped due to token limit\")\n",
        "            continue\n",
        "\n",
        "        prompt = [\n",
        "            ChatMessage(role=\"system\", content='Génère une sortie JSON avec les clés suivantes et leurs valeurs respectives à partir des informations que je te donne. Si la clé est absente, le champ est rempli avec une chaîne vide ou apparaît comme « NaN » (Not a Number) dans le jeu de données : \"vendor Email\" : adresse e-mail du fournisseur. \"numeroPiece\" : Contient la référence (Réf.) du document. S’il s’agit d’une situation de travaux, ajoute à cette référence le numéro de situation après un slash. \"typeDocument\" : Spécifie le type de document en fonction de la typologie suivante : (09.01 - Factures / 09.04 - Factures développement (avant promesse) / 09.07 - Factures marketing / 09.08 - Factures travaux et prorata / 09.21 - Honoraires juridiques / 09.22 - Honoraires prescripteurs / 09.31 - Situations travaux MOE BPCC / 09.32 - Situations travaux MOE externe. »). \"tiersSiret\" : Un numéro SIRET français unique à chaque entreprise française. \"codeProjet\" : Représente un code unique pour le projet associé à la facture ; composé de 14 chiffres : les 9 chiffres du Siren et 5 chiffres propres à l’établissement. \"ttc\" : Le montant total à payer, toutes taxes comprises (TTC). \"tiers\" : Le nom du tiers ou du fournisseur. Vigilance sour le taux de TVA appliqué car il y a de forte variabilité, tu dois te Baer sur le total TTC. \"ht\" : Le montant total du document au format hors taxes (HT). \"dateDocument\" : La date d’emission du document. \"dueDate\" : Indique la date de l échéance du document. Elle est fixée à 30 jours pour une facture, 45 ou 60 pour les engagements de travaux. En fonction de la date de facture, elle sera payée en milieu de mois (le 15) ou ou fin du mois (30 ou 31) suivant la date d’émission de la facture. Il faut compter 15 jours de plus pour les situations de travaux (1 mois 15 jours et maximum 2 mois). \"typeFacture\" : Indique le type de la facture (facture, situation de travaux ou avoir).'),\n",
        "            ChatMessage(role=\"user\", content=f\"Here are the document details: \\n{document_details}, create a JSON output\")\n",
        "        ]\n",
        "\n",
        "        completion = client.chat.completions.create(model=model, response_format={ \"type\": \"json_object\" }, messages=prompt)\n",
        "        llm_output = completion.choices[0].message.content\n",
        "        invoice_data[\"LLM_output\"] = llm_output\n",
        "        results.append(invoice_data)\n",
        "\n",
        "    with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\n",
        "            \"sentence\", \"file_name\", \"LLM_output\"\n",
        "        ]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for item in results:\n",
        "            writer.writerow(item)\n",
        "\n",
        "process_json_files(directory_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "NDOBXqz24aXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}